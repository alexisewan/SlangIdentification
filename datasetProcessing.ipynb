{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datasetProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drivePath = 'drive/MyDrive/Colab Notebooks/finalProj/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQgvFnL_yPoD",
        "outputId": "66064a78-94a6-4c54-d666-435b489617ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S20WJeYT0Og"
      },
      "outputs": [],
      "source": [
        "#kaggle API key, to get UD dataset.\n",
        "! echo \"{\\\"username\\\":\\\"ericclark\\\",\\\"key\\\":\\\"d84149862033d3369bad2f997df47d26\\\"}\" > kaggle.json\n",
        "\n",
        "!pip install requests\n",
        "! pip install kaggle\n",
        "! pip install transformers\n",
        "! pip install tensorflow\n",
        "\n",
        "#getting the urban dictionary words datat set from Kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download therohk/urban-dictionary-words-dataset\n",
        "! unzip urban-dictionary-words-dataset\n",
        "# gives urbandict-word-defs.csv - for use in defining slang terms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head urbandict-word-defs.csv\n",
        "import csv\n",
        "file = open(\"urbandict-word-defs.csv\")\n",
        "csvreader = csv.reader(file)\n",
        "header = next(csvreader)\n",
        "print(header)\n",
        "udWords = []\n",
        "for row in csvreader:\n",
        "    udWords.append(row)\n",
        "print(len(udWords))\n",
        "file.close()"
      ],
      "metadata": {
        "id": "oGS7R4gsUokV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build the slang example sentence dataset.\n",
        "# random.randint(0, 9)\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "datasetSize = 13000\n",
        "sampleWords = random.sample(udWords, datasetSize)\n",
        "#['word_id', 'word', 'up_votes', 'down_votes', 'author', 'definition']\n",
        "response = 0\n",
        "examples = []\n",
        "for sampleWord in sampleWords:\n",
        "    time.sleep(.1)\n",
        "    response = requests.get(\"https://api.urbandictionary.com/v0/define?defid=\"+sampleWord[0])\n",
        "    apiData = response.json()['list']\n",
        "    # skip words with no definition\n",
        "    if(len(apiData) == 0):\n",
        "      continue\n",
        "    apiData = apiData[0]\n",
        "    word = sampleWord[1]\n",
        "    #print(apiData)\n",
        "    example = apiData['example']\n",
        "    examples.append([word, example])\n",
        "print(len(examples))\n",
        "  # call the urban dictionary api\n",
        "  #sleep for .25 seconds\n",
        "  # add the sentence to the doc.\n",
        "  # add the tags to the tagset\n",
        "\n",
        "# convert to dictionary, split sample data on double newline (how the example sentences are split in the api response.)\n",
        "exampleDict = {}\n",
        "\n",
        "for row in examples:\n",
        "  splixEx = row[1].split(\"\\n\\n\")\n",
        "  exampleDict[row[0]] = splixEx\n",
        "#print(exampleDict)\n",
        "# create a binary pickle file \n",
        "f = open(\"drive/MyDrive/Colab Notebooks/finalProj/examples.pkl\",\"wb\")\n",
        "\n",
        "# write the python object (dict) to pickle file, to save it.\n",
        "pickle.dump(exampleDict,f)"
      ],
      "metadata": {
        "id": "hXbw4_v9T8uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we end up tokenizing the data, do it here.\n",
        "# input format is:\n",
        "# dictionary {\n",
        "# 'vocab word' : ['example sentence 1', 'example sentence 2', etc.]\n",
        "# ...   \n",
        "# }\n",
        "#\n",
        "# ouytput format is:\n",
        "# dictionary {\n",
        "# 'vocab word' : {'keyToken': ['tokenized', 'key'], 'examples': ['example sentence 1', 'example sentence 2', etc.]}\n",
        "# ...   \n",
        "# }\n",
        "#\n",
        "def tokenize(dataDict):\n",
        "  tokenDict = {}\n",
        "  # for now our simple tokenization will be to split on whitespace.\n",
        "  for key, exampleSents in exampleDict.items():\n",
        "    for index, sentence in enumerate(exampleSents):\n",
        "      exampleSents[index] = sentence.split()\n",
        "    tokenDict[key.lower()] = {'keyToken' : key.lower().split(), 'examples' : exampleSents}\n",
        "  return tokenDict\n",
        "\n",
        "# TODO: when labeling the example sentences, we really should be checking for every single slang word in our vocab, rather than just checking for the example.\n",
        "# This may cause issues with detecting slang properly, or scoring our data.\n",
        "def label(dataDict):\n",
        "  labeledDict = {}\n",
        "  for key, keyData in dataDict.items():\n",
        "    exampleLabels = []\n",
        "    for index, sentence in enumerate(keyData['examples']):\n",
        "      labels = ['O'] * len(sentence)\n",
        "      keyLoc = find_subarray(sentence, keyData['keyToken'])\n",
        "      if keyLoc is not None:\n",
        "        for idx, location in enumerate(keyLoc):\n",
        "          # the word is in the sentence.\n",
        "          # tag that section as slang.\n",
        "          keyLen = len(keyData['keyToken'])\n",
        "          for i in range(keyLen):\n",
        "            slangLabel = 'I-SLANG'\n",
        "            if i == 0:\n",
        "              slangLabel = 'B-SLANG'\n",
        "            labels[location + i] = slangLabel\n",
        "        exampleLabels.append(labels)\n",
        "    labeledDict[key] = {'keyToken': keyData['keyToken'], 'examples': keyData['examples'], 'labels': exampleLabels}\n",
        "  return labeledDict\n",
        "\n",
        "# from internet.\n",
        "# Code to find the index of a subarray.\n",
        "def find_subarray(a, b):\n",
        "    return [x for x in range(len(a)) if a[x:x+len(b)] == b]"
      ],
      "metadata": {
        "id": "lBsA08XlruAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import re\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/finalProj/examples.pkl\",\"rb\")\n",
        "exampleDict = pickle.load(f)\n",
        "f.close()\n",
        "filterDict = {}\n",
        "# Regularize the examples, removing non alphanumeric, lowercasing everything, and stripping whitespace.\n",
        "for key, exampleSents in exampleDict.items():\n",
        "  for index, sentence in enumerate(exampleSents):\n",
        "    sentence = re.sub(r'[^a-zA-Z0-9-\\s]', ' ', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    key = re.sub(r'[^a-zA-Z0-9-\\s]', ' ', key)\n",
        "    key = key.lower()\n",
        "    key = key.strip()\n",
        "    exampleSents[index] = sentence\n",
        "  filterDict[key] = exampleSents\n",
        "\n",
        "# If we do a tokenizing step, it happens here.\n",
        "tokenizedDict = tokenize(filterDict)\n",
        "# build out the labels\n",
        "labeledDict = label(tokenizedDict)\n",
        "\n",
        "filehandler = open(\"drive/MyDrive/Colab Notebooks/finalProj/labelDict.txt\", 'wt')\n",
        "data = str(labeledDict)\n",
        "filehandler.write(data)\n",
        "filehandler.close()\n",
        "\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/finalProj/labelDict.pkl\",\"wb\")\n",
        "\n",
        "# write the python object (dict) to pickle file, to save it.\n",
        "pickle.dump(labeledDict,f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "iPiiVCnRzTDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# tokenize, configure and set up the slang negative examples.\n",
        "#using the test/train datasets from assignment 2 as our negative examples.\n",
        "# we are combining the test and train data into one, since we will be performing a split later in our pipeline.\n",
        "negSamples = []\n",
        "with open(drivePath + 'train.txt') as trainData:\n",
        "  lines = trainData.readlines()\n",
        "with open(drivePath + 'test.txt') as trainData:\n",
        "  lines += trainData.readlines()\n",
        "for line in lines:\n",
        "  line = re.sub(r'[^a-zA-Z0-9-\\s]', ' ', line)\n",
        "  line = line.lower()\n",
        "  line = line.strip()\n",
        "  #TODO: if we use fancier tokenization, it will need to be implemented here too\n",
        "  tokenLine = line.split()\n",
        "  negSamples.append({'sent': tokenLine, 'label': ['O'] * len(tokenLine)})"
      ],
      "metadata": {
        "id": "WgIN4uJC1rAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# for ease of processing, convert the labeledDict (positive examples) into the same format as the negative examples, and combine them.\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/finalProj/labelDict.pkl\",\"rb\")\n",
        "labelDict = pickle.load(f)\n",
        "slangSamples = []\n",
        "for key, slangData in labelDict.items():\n",
        "  for i in range(len(slangData['examples'])):\n",
        "    slangSamples.append({'sent': slangData['examples'][i], 'label': slangData['labels'][i]})"
      ],
      "metadata": {
        "id": "ckMHTz0wSPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "fullSamples = slangSamples + negSamples\n",
        "with open(drivePath + \"fullSamples.pkl\",\"wb\") as full:\n",
        "  pickle.dump(fullSamples, full)\n",
        "with open(drivePath + \"slangSamples.pkl\",\"wb\") as slang:\n",
        "  pickle.dump(slangSamples, slang)\n",
        "with open(drivePath + \"negSamples.pkl\",\"wb\") as neg:\n",
        "  pickle.dump(negSamples, neg)\n",
        "print(fullSamples[-1])"
      ],
      "metadata": {
        "id": "F-Vi-nRsTHq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/finalProj/examples.pkl\",\"rb\")\n",
        "exampleDict = pickle.load(f)\n",
        "f.close()\n",
        "count = 0\n",
        "for key, exampleSents in exampleDict.items():\n",
        "  if \"-\" in key:\n",
        "    count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "BUI6rSc5CA3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}